#Calculating self-attention

x*Wq=q
x*Wk=k
x*Wv=v
W is the weight
e.g. x1 * Wq=q1


===========================

Step1: Create three vectors from each of the encoder’s input vectors:
a. Quary Vector
b. Key Vector
c. Value Vector

Step2: calculate self-attention for every word in the input sequence:

e.g. Action gets results

a.To calculate the self-attention for the first word “Action”, we will calculate scores for all the words in the phrase with respect 
to “Action”. This score determines the importance of other words when we are encoding a certain word in an input sequence

| Words         | Q vector| K vector| V vector| Score  | 
| ------------- |:-------:| :------:| :------:|:------:|
| Action        |   q1    |     k1  |    v1   |  q1k1  |
| gets          |         |     k2  |    v2   |  q1k2  |
| results       |         |     k3  |    v3   |  q1k3  |

b. Then, these scores are divided by 8 which is the square root of the dimension of the key vector:
 #divided by  d−−√  to make the scores less sensitive to the dimension  d 

| Words         | Q vector| K vector| V vector| Score  | Score  |
| ------------- |:-------:| :------:| :------:|:------:|:------:|
| Action        |   q1    |     k1  |    v1   |  q1k1  | q1k1/8 |
| gets          |         |     k2  |    v2   |  q1k2  | q1k2/8 |
| results       |         |     k3  |    v3   |  q1k3  | q1k3/8 |

c. Using the softmax activation function to normalize those scores

| Words         | Q vector| K vector| V vector| Score  | Score  | Softmax|
| ------------- |:-------:| :------:| :------:|:------:|:------:|:------:|
| Action        |   q1    |     k1  |    v1   |  q1k1  | q1k1/8 |   x11  |
| gets          |         |     k2  |    v2   |  q1k2  | q1k2/8 |   x12  |
| results       |         |     k3  |    v3   |  q1k3  | q1k3/8 |   x13  |

d. These normalized scores are then multiplied by the value vectors (v1, v2, v3) and sum up the resultant vectors to arrive at the final vector (z1). This is the output of the self-attention layer. It is then passed on to the feed-forward network as input:

| Words         | Q vector| K vector| V vector| Score  | Score  | Softmax|Softmax * v|    Sum   |
| ------------- |:-------:| :------:| :------:|:------:|:------:|:------:|:---------:|:--------:|
| Action        |   q1    |     k1  |    v1   |  q1k1  | q1k1/8 |   x11  |   x11 *v1 |    z1    |
| gets          |         |     k2  |    v2   |  q1k2  | q1k2/8 |   x12  |   x12 *v2 |          |
| results       |         |     k3  |    v3   |  q1k3  | q1k3/8 |   x13  |   x13 *v3 |          |

z1=(x11*v1+x12*v2+x13*v3)

e. z1 is the self-attention vector for the first word of the input sequence “Action gets results”. We can get the vectors for the rest of the words in the input sequence in the same fashion:

| Words         | Q vector| K vector| V vector| Score  | Score  | Softmax|Softmax * v|    Sum   |
| ------------- |:-------:| :------:| :------:|:------:|:------:|:------:|:---------:|:--------:|
| Action        |         |     k1  |    v1   |  q2k1  | q2k1/8 |   x21  |   x21 *v1 |          |
| gets          |   q2    |     k2  |    v2   |  q2k2  | q2k2/8 |   x22  |   x22 *v2 |   z2     |
| results       |         |     k3  |    v3   |  q2k3  | q2k3/8 |   x23  |   x23 *v3 |          |
z2=(x21*v1+x22*v2+x23*v3)


| Words         | Q vector| K vector| V vector| Score  | Score  | Softmax|Softmax * v|    Sum   |
| ------------- |:-------:| :------:| :------:|:------:|:------:|:------:|:---------:|:--------:|
| Action        |         |     k1  |    v1   |  q3k1  | q3k1/8 |   x31  |   x31 *v1 |          |
| gets          |         |     k2  |    v2   |  q3k2  | q3k2/8 |   x32  |   x32 *v2 |          |
| results       |    q3   |     k3  |    v3   |  q3k3  | q3k3/8 |   x33  |   x33 *v3 |     z3   |
z3=(x31*v1+x32*v2+x33*v3)


Self-attention is computed not once but multiple times in the Transformer’s architecture, in parallel and independently. It is therefore referred to as Multi-head Attention. The outputs are concatenated and linearly transformed. 
#https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/
#https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms

